{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Dreem classification d'états de sommeil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auteurs : Antoine Bourgoin et Benoit Charmettant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cadre du cours de machine learning, il nous était proposé de participer à un challenge organisé par la société Dreem qui développe un bandeau connecté permettant à chacun de d'analyser son sommeil. Ils ont mis à notre disposition un jeu de données issues de leur bandeau et in nous était demandé de proposer un algorithme permettant de classifier des séquences d'enregistrement des différents capteurs du bandeau selon les différents état de sommeils connus et utilisés en pratique. Le tout était proposé sous le format d'un challenge entre les étudiants sur le site spécialisé Kaggle.\n",
    "Le code que nous avons écrit étant relativement important nous avons du l'organiser dans plusieurs fichiers et sous fichiers pour plus de clareté (tous sont disponibles). Les points d'entrées sont les deux notebook l'un portant sur l'aspect extraction des features et l'autre sur l'aspect prédiction de l'état de sommeil d'après ces features. Ces deux notebook utilisent des fonctions outils réparties dans les autres fichiers qu'il n'est pas forcément nécessaire pour une compréhension globale du travail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description du problème à traiter\n",
    "\n",
    "### Problème biologique\n",
    "\n",
    "### Etat de l'art\n",
    "Citer qques articles pour montrer qu'on en a lu et que le sujet a déjà été abordé par d'autres\n",
    "### Données proposées \n",
    "Inclure un descriptif des données + peut être quelques visualisations obtenues (typiquement les diagrammes en boite qui nous ont aidées à choisir les features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode\n",
    "Pour définir notre méthode de travail nous avons décomposé le travail initialement en plusieurs étapes que nous allons détailler ici. Après avoir importé les données et les avoir formatées \n",
    "### Calculs des features\n",
    "\n",
    "### Description des features\n",
    "\n",
    "Petite listes des grandes catégories de features que nous avons calculée et pourquoi celles ci.\n",
    "\n",
    "### Sélection et traitement des features\n",
    "\n",
    "### Sélection du modèle\n",
    "\n",
    "### Stratégie d'expérimentation dans le cadre d'un challenge\n",
    "Etant donné que le travail se déroulait dans le contexte d'un challenge nous avons adapté notre méthode en conséquence. Nous avons voulu mettre en place un pipeline de données, suivant les étapes décrites précédemment, fonctionnel dès le début en commençant par une approche naïve de chaque étape avec pour objectif d'affiner chacune des étapes au fur et à mesure pour améliorer nos résultats. Cela nous permettait d'avoir des résultats rapidement et d'itérer fréquement pour améliorer nos performances et notre classement progressivement. Nous nous sommes vite aperçu que la quantité à traiter et nos moyens matériels limités ne nous permettraient pas de n'utiliser qu'un seul script que nous executerions dans son ensemble à chaque fois. Par exemple le calcule de quelques features simples sur tous les EEG des données d'entrainement et d'évaluation nécéssitait au minimum 2h30/3h de calculs sur nos ordinateurs. Nous avons donc ségmenté le travail en deux, un script dédié au calculs des features que nous enregistrons sur des fichiers en local de manière à ne pas avoir à calculer la même features deux fois. Et un script dédié au modèle prédictif qui charge les features précédemment calculées et implémente toutes les étapes jusqu'à la production d'un fichier réponse directement chargeable sur Kaggle. Nous avons pris le temps de développer des fonctions outils modulables nous permettant d'implémenter rapidement une nouvelle features, de la calculer sur les données que nous choisissions et de la charger facilement pour l'associer aux autres features déjà calculées. Nous le précisons car ce fut une part non négligeable de notre travail, qui pourrait dans un contexte de vie réelle être publié et réutilisé par d'autres équipes de recherches par exemple. Enfin nous avons mis en place un système nous permettant d'utiliser des capacités de calcul plus importantes que nos ordinateurs via le service Google Colab. Cela nous a permis de réduire d'environ 30% le temps de calculs des features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "On aurait pu aller vers du deep learning pour l'extraction de features : on ne l'a pas fait, cela nous semblait moins pertinent au regard du cours que nous avions suivi, bien qu'il y ai de bonnes chances que les techniques d'encodage par réseaux de neurones \n",
    "\n",
    "Effet boite noire du random forest ?\n",
    "\n",
    "Finalement comme nous travaillons dans le cadre d'un challenge avec un temps limité à dédier à ce projet (étant donné le travail que nous devons fournir pour d'autres matières et d'autres projets) nous avons tout de même adopté une démarche nous permettant d'avoir rapidement des résultats compétitifs. Or il se trouve que la technique de Random Forest nous a permis d'avoir des résultats convainquants et qui s'amélioraient au fur et à meusure que nous lui ajoutions de nouvelles features. Nous avons tout de même tenté d'autres approches et essayé d'affiner notre méthode en essayant d'autres modèles plus parlants, en selectionnant plus habilement les features utilisées. Force est de constater que cela n'améliorait pas les résultats obtenu par une démarche plus \"brutale\" et moins explicite : celle de donner un maximum de features à un modèle Random Forest. C'est un peu décevant, mais l'exercice nous contraignait un peu à ce niveau là. Ceci-dit, nous pensons qu'une démarche plus fine porterait très probablement ses fruits, nous péchons aussi par manque de temps (et de capacités matérielles, les calculs prenant beaucoup de temps sur des données de cette taille ce qui a tout de même \n",
    "\n",
    "Perspectives et continuation du travail\n",
    "\n",
    "CCls personnelles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreem_challenge",
   "language": "python",
   "name": "dreem_challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
